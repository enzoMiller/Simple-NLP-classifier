<h2><a href=http://arxiv.org/pdf/1502.05803v3.pdf>Visual object tracking performance measures revisited</a></h2>
<p>  The problem of visual tracking evaluation is sporting a large variety of
performance measures, and largely suffers from lack of consensus about which
measures should be used in experiments. This makes the cross-paper tracker
comparison difficult. Furthermore, as some measures may be less effective than
others, the tracking results may be skewed or biased towards particular
tracking aspects. In this paper we revisit the popular performance measures and
tracker performance visualizations and analyze them theoretically and
experimentally. We show that several measures are equivalent from the point of
information they provide for tracker comparison and, crucially, that some are
more brittle than the others. Based on our analysis we narrow down the set of
potential measures to only two complementary ones, describing accuracy and
robustness, thus pushing towards homogenization of the tracker evaluation
methodology. These two measures can be intuitively interpreted and visualized
and have been employed by the recent Visual Object Tracking (VOT) challenges as
the foundation for the evaluation methodology.
</p>
<h2><a href=http://arxiv.org/pdf/1304.5315v1.pdf>Quality-Aware Coding and Relaying for 60 GHz Real-Time Wireless Video
  Broadcasting</a></h2>
<p>  Wireless streaming of high-definition video is a promising application for 60
GHz links, since multi-Gigabit/s data rates are possible. In particular we
consider a sports stadium broadcasting system where video signals from multiple
cameras are transmitted to a central location. Due to the high pathloss of
60\,GHz radiation over the large distances encountered in this setting, the use
of relays is required. This paper designs a quality-aware coding and relaying
algorithm for maximization of the overall video quality. We consider the
setting that the source can split its data stream into parallel streams, which
can be transmitted via different relays to the destination. For this, we derive
the related formulation and re-formulate it as convex programming, which can
guarantee optimal solutions.
</p>
<h2><a href=http://arxiv.org/pdf/1009.4991v1.pdf>Web Page Categorization Using Artificial Neural Networks</a></h2>
<p>  Web page categorization is one of the challenging tasks in the world of ever
increasing web technologies. There are many ways of categorization of web pages
based on different approach and features. This paper proposes a new dimension
in the way of categorization of web pages using artificial neural network (ANN)
through extracting the features automatically. Here eight major categories of
web pages have been selected for categorization; these are business & economy,
education, government, entertainment, sports, news & media, job search, and
science. The whole process of the proposed system is done in three successive
stages. In the first stage, the features are automatically extracted through
analyzing the source of the web pages. The second stage includes fixing the
input values of the neural network; all the values remain between 0 and 1. The
variations in those values affect the output. Finally the third stage
determines the class of a certain web page out of eight predefined classes.
This stage is done using back propagation algorithm of artificial neural
network. The proposed concept will facilitate web mining, retrievals of
information from the web and also the search engines.
</p>
<h2><a href=http://arxiv.org/pdf/1506.08898v3.pdf>Low-latency compression of mocap data using learned spatial
  decorrelation transform</a></h2>
<p>  Due to the growing needs of human motion capture (mocap) in movie, video
games, sports, etc., it is highly desired to compress mocap data for efficient
storage and transmission. This paper presents two efficient frameworks for
compressing human mocap data with low latency. The first framework processes
the data in a frame-by-frame manner so that it is ideal for mocap data
streaming and time critical applications. The second one is clip-based and
provides a flexible tradeoff between latency and compression performance. Since
mocap data exhibits some unique spatial characteristics, we propose a very
effective transform, namely learned orthogonal transform (LOT), for reducing
the spatial redundancy. The LOT problem is formulated as minimizing square
error regularized by orthogonality and sparsity and solved via alternating
iteration. We also adopt a predictive coding and temporal DCT for temporal
decorrelation in the frame- and clip-based frameworks, respectively.
Experimental results show that the proposed frameworks can produce higher
compression performance at lower computational cost and latency than the
state-of-the-art methods.
</p>
<h2><a href=http://arxiv.org/pdf/1506.07271v1.pdf>Natural Scene Recognition Based on Superpixels and Deep Boltzmann
  Machines</a></h2>
<p>  The Deep Boltzmann Machines (DBM) is a state-of-the-art unsupervised learning
model, which has been successfully applied to handwritten digit recognition
and, as well as object recognition. However, the DBM is limited in scene
recognition due to the fact that natural scene images are usually very large.
In this paper, an efficient scene recognition approach is proposed based on
superpixels and the DBMs. First, a simple linear iterative clustering (SLIC)
algorithm is employed to generate superpixels of input images, where each
superpixel is regarded as an input of a learning model. Then, a two-layer DBM
model is constructed by stacking two restricted Boltzmann machines (RBMs), and
a greedy layer-wise algorithm is applied to train the DBM model. Finally, a
softmax regression is utilized to categorize scene images. The proposed
technique can effectively reduce the computational complexity and enhance the
performance for large natural image recognition. The approach is verified and
evaluated by extensive experiments, including the fifteen-scene categories
dataset the UIUC eight-sports dataset, and the SIFT flow dataset, are used to
evaluate the proposed method. The experimental results show that the proposed
approach outperforms other state-of-the-art methods in terms of recognition
rate.
</p>
<h2><a href=http://arxiv.org/pdf/1506.01929v2.pdf>Learning to track for spatio-temporal action localization</a></h2>
<p>  We propose an effective approach for spatio-temporal action localization in
realistic videos. The approach first detects proposals at the frame-level and
scores them with a combination of static and motion CNN features. It then
tracks high-scoring proposals throughout the video using a
tracking-by-detection approach. Our tracker relies simultaneously on
instance-level and class-level detectors. The tracks are scored using a
spatio-temporal motion histogram, a descriptor at the track level, in
combination with the CNN features. Finally, we perform temporal localization of
the action using a sliding-window approach at the track level. We present
experimental results for spatio-temporal localization on the UCF-Sports, J-HMDB
and UCF-101 action localization datasets, where our approach outperforms the
state of the art with a margin of 15%, 7% and 12% respectively in mAP.
</p>
<h2><a href=http://arxiv.org/pdf/1506.00051v1.pdf>Bag-of-Genres for Video Genre Retrieval</a></h2>
<p>  This paper presents a higher level representation for videos aiming at video
genre retrieval. In video genre retrieval, there is a challenge that videos may
comprise multiple categories, for instance, news videos may be composed of
sports, documentary, and action. Therefore, it is interesting to encode the
distribution of such genres in a compact and effective manner. We propose to
create a visual dictionary using a genre classifier. Each visual word in the
proposed model corresponds to a region in the classification space determined
by the classifier's model learned on the training frames. Therefore, the video
feature vector contains a summary of the activations of each genre in its
contents. We evaluate the bag-of-genres model for video genre retrieval, using
the dataset of MediaEval Tagging Task of 2012. Results show that the proposed
model increases the quality of the representation being more compact than
existing features.
</p>
<h2><a href=http://arxiv.org/pdf/1408.7071v1.pdf>Temporal Extension of Scale Pyramid and Spatial Pyramid Matching for
  Action Recognition</a></h2>
<p>  Historically, researchers in the field have spent a great deal of effort to
create image representations that have scale invariance and retain spatial
location information. This paper proposes to encode equivalent temporal
characteristics in video representations for action recognition. To achieve
temporal scale invariance, we develop a method called temporal scale pyramid
(TSP). To encode temporal information, we present and compare two methods
called temporal extension descriptor (TED) and temporal division pyramid (TDP)
. Our purpose is to suggest solutions for matching complex actions that have
large variation in velocity and appearance, which is missing from most current
action representations. The experimental results on four benchmark datasets,
UCF50, HMDB51, Hollywood2 and Olympic Sports, support our approach and
significantly outperform state-of-the-art methods. Most noticeably, we achieve
65.0% mean accuracy and 68.2% mean average precision on the challenging HMDB51
and Hollywood2 datasets which constitutes an absolute improvement over the
state-of-the-art by 7.8% and 3.9%, respectively.
</p>
<h2><a href=http://arxiv.org/pdf/1102.5225v4.pdf>Let Us Dance Just a Little Bit More --- On the Information Capacity of
  the Human Motor System</a></h2>
<p>  Fitts' law is a fundamental tool in measuring the capacity of the human motor
system. However, it is, by definition, limited to aimed movements toward
spatially expanded targets. We revisit its information-theoretic basis with the
goal of generalizing it into unconstrained trained movement such as dance and
sports. The proposed new measure is based on a subject's ability to accurately
reproduce a complex movement pattern. We demonstrate our framework using
motion-capture data from professional dance performances.
</p>
<h2><a href=http://arxiv.org/pdf/1411.6496v1.pdf>Automatic Summarization of Soccer Highlights Using Audio-visual
  Descriptors</a></h2>
<p>  Automatic summarization generation of sports video content has been object of
great interest for many years. Although semantic descriptions techniques have
been proposed, many of the approaches still rely on low-level video descriptors
that render quite limited results due to the complexity of the problem and to
the low capability of the descriptors to represent semantic content. In this
paper, a new approach for automatic highlights summarization generation of
soccer videos using audio-visual descriptors is presented. The approach is
based on the segmentation of the video sequence into shots that will be further
analyzed to determine its relevance and interest. Of special interest in the
approach is the use of the audio information that provides additional
robustness to the overall performance of the summarization system. For every
video shot a set of low and mid level audio-visual descriptors are computed and
lately adequately combined in order to obtain different relevance measures
based on empirical knowledge rules. The final summary is generated by selecting
those shots with highest interest according to the specifications of the user
and the results of relevance measures. A variety of results are presented with
real soccer video sequences that prove the validity of the approach.
</p>
<h2><a href=http://arxiv.org/pdf/1303.6021v1.pdf>Spatio-Temporal Covariance Descriptors for Action and Gesture
  Recognition</a></h2>
<p>  We propose a new action and gesture recognition method based on
spatio-temporal covariance descriptors and a weighted Riemannian locality
preserving projection approach that takes into account the curved space formed
by the descriptors. The weighted projection is then exploited during boosting
to create a final multiclass classification algorithm that employs the most
useful spatio-temporal regions. We also show how the descriptors can be
computed quickly through the use of integral video representations. Experiments
on the UCF sport, CK+ facial expression and Cambridge hand gesture datasets
indicate superior performance of the proposed method compared to several recent
state-of-the-art techniques. The proposed method is robust and does not require
additional processing of the videos, such as foreground detection,
interest-point detection or tracking.
</p>
<h2><a href=http://arxiv.org/pdf/1204.6321v1.pdf>Efficient Video Indexing on the Web: A System that Leverages User
  Interactions with a Video Player</a></h2>
<p>  In this paper, we propose a user-based video indexing method, that
automatically generates thumbnails of the most important scenes of an online
video stream, by analyzing users' interactions with a web video player. As a
test bench to verify our idea we have extended the YouTube video player into
the VideoSkip system. In addition, VideoSkip uses a web-database (Google
Application Engine) to keep a record of some important parameters, such as the
timing of basic user actions (play, pause, skip). Moreover, we implemented an
algorithm that selects representative thumbnails. Finally, we populated the
system with data from an experiment with nine users. We found that the
VideoSkip system indexes video content by leveraging implicit users
interactions, such as pause and thirty seconds skip. Our early findings point
toward improvements of the web video player and its thumbnail generation
technique. The VideSkip system could compliment content-based algorithms, in
order to achieve efficient video-indexing in difficult videos, such as lectures
or sports.
</p>
<h2><a href=http://arxiv.org/pdf/1109.6199v1.pdf>The Aware Cricket Ground</a></h2>
<p>  The most profound technologies are those that disappear. They weave
themselves into fabrics of everyday life until they are indistinguishable from
it [1]. This research work is a mere effort for automated decision making
during sports of most common interest leveraging ubiquitous computing.
Primarily cricket has been selected for the first implementation of the idea. A
positioning system is used for locating the objects moving in the field. Main
objectives of the research are to help achieve the following goals. 1) Make
Decisions where human eye can make error due to human limitations. 2) Simulate
the Match activity during and after the game in a 3D computerized Graphics
system. 3) Make various types of game and performance analysis of a certain
team or a player.
</p>
<h2><a href=http://arxiv.org/pdf/1001.0442v1.pdf>Modeling and Annotating the Expressive Semantics of Dance Videos</a></h2>
<p>  Dance videos are interesting and semantics-intensive. At the same time, they
are the complex type of videos compared to all other types such as sports, news
and movie videos. In fact, dance video is the one which is less explored by the
researchers across the globe. Dance videos exhibit rich semantics such as macro
features and micro features and can be classified into several types. Hence,
the conceptual modeling of the expressive semantics of the dance videos is very
crucial and complex. This paper presents a generic Dance Video Semantics Model
(DVSM) in order to represent the semantics of the dance videos at different
granularity levels, identified by the components of the accompanying song. This
model incorporates both syntactic and semantic features of the videos and
introduces a new entity type called, Agent, to specify the micro features of
the dance videos. The instantiations of the model are expressed as graphs. The
model is implemented as a tool using J2SE and JMF to annotate the macro and
micro features of the dance videos. Finally examples and evaluation results are
provided to depict the effectiveness of the proposed dance video model.
Keywords: Agents, Dance videos, Macro features, Micro features, Video
annotation, Video semantics.
</p>
<h2><a href=http://arxiv.org/pdf/0912.2302v1.pdf>Synthesis of supervised classification algorithm using intelligent and
  statistical tools</a></h2>
<p>  A fundamental task in detecting foreground objects in both static and dynamic
scenes is to take the best choice of color system representation and the
efficient technique for background modeling. We propose in this paper a
non-parametric algorithm dedicated to segment and to detect objects in color
images issued from a football sports meeting. Indeed segmentation by pixel
concern many applications and revealed how the method is robust to detect
objects, even in presence of strong shadows and highlights. In the other hand
to refine their playing strategy such as in football, handball, volley ball,
Rugby..., the coach need to have a maximum of technical-tactics information
about the on-going of the game and the players. We propose in this paper a
range of algorithms allowing the resolution of many problems appearing in the
automated process of team identification, where each player is affected to his
corresponding team relying on visual data. The developed system was tested on a
match of the Tunisian national competition. This work is prominent for many
next computer vision studies as it's detailed in this study.
</p>
<h2><a href=http://arxiv.org/pdf/1702.04829v1.pdf>Sports stars: analyzing the performance of astronomers at
  visualization-based discovery</a></h2>
<p>  In this data-rich era of astronomy, there is a growing reliance on automated
techniques to discover new knowledge. The role of the astronomer may change
from being a discoverer to being a confirmer. But what do astronomers actually
look at when they distinguish between "sources" and "noise?" What are the
differences between novice and expert astronomers when it comes to visual-based
discovery? Can we identify elite talent or coach astronomers to maximize their
potential for discovery? By looking to the field of sports performance
analysis, we consider an established, domain-wide approach, where the expertise
of the viewer (i.e. a member of the coaching team) plays a crucial role in
identifying and determining the subtle features of gameplay that provide a
winning advantage. As an initial case study, we investigate whether the
SportsCode performance analysis software can be used to understand and document
how an experienced HI astronomer makes discoveries in spectral data cubes. We
find that the process of timeline-based coding can be applied to spectral cube
data by mapping spectral channels to frames within a movie. SportsCode provides
a range of easy to use methods for annotation, including feature-based codes
and labels, text annotations associated with codes, and image-based drawing.
The outputs, including instance movies that are uniquely associated with coded
events, provide the basis for a training program or team-based analysis that
could be used in unison with discipline specific analysis software. In this
coordinated approach to visualization and analysis, SportsCode can act as a
visual notebook, recording the insight and decisions in partnership with
established analysis methods. Alternatively, in situ annotation and coding of
features would be a valuable addition to existing and future visualisation and
analysis packages.
</p>
<h2><a href=http://arxiv.org/pdf/1607.02003v1.pdf>Tubelets: Unsupervised action proposals from spatiotemporal super-voxels</a></h2>
<p>  This paper considers the problem of localizing actions in videos as a
sequences of bounding boxes. The objective is to generate action proposals that
are likely to include the action of interest, ideally achieving high recall
with few proposals. Our contributions are threefold. First, inspired by
selective search for object proposals, we introduce an approach to generate
action proposals from spatiotemporal super-voxels in an unsupervised manner, we
call them Tubelets. Second, along with the static features from individual
frames our approach advantageously exploits motion. We introduce independent
motion evidence as a feature to characterize how the action deviates from the
background and explicitly incorporate such motion information in various stages
of the proposal generation. Finally, we introduce spatiotemporal refinement of
Tubelets, for more precise localization of actions, and pruning to keep the
number of Tubelets limited. We demonstrate the suitability of our approach by
extensive experiments for action proposal quality and action localization on
three public datasets: UCF Sports, MSR-II and UCF101. For action proposal
quality, our unsupervised proposals beat all other existing approaches on the
three datasets. For action localization, we show top performance on both the
trimmed videos of UCF Sports and UCF101 as well as the untrimmed videos of
MSR-II.
</p>
<h2><a href=http://arxiv.org/pdf/1605.08125v1.pdf>Automatic Action Annotation in Weakly Labeled Videos</a></h2>
<p>  Manual spatio-temporal annotation of human action in videos is laborious,
requires several annotators and contains human biases. In this paper, we
present a weakly supervised approach to automatically obtain spatio-temporal
annotations of an actor in action videos. We first obtain a large number of
action proposals in each video. To capture a few most representative action
proposals in each video and evade processing thousands of them, we rank them
using optical flow and saliency in a 3D-MRF based framework and select a few
proposals using MAP based proposal subset selection method. We demonstrate that
this ranking preserves the high quality action proposals. Several such
proposals are generated for each video of the same action. Our next challenge
is to iteratively select one proposal from each video so that all proposals are
globally consistent. We formulate this as Generalized Maximum Clique Graph
problem using shape, global and fine grained similarity of proposals across the
videos. The output of our method is the most action representative proposals
from each video. Our method can also annotate multiple instances of the same
action in a video. We have validated our approach on three challenging action
datasets: UCF Sport, sub-JHMDB and THUMOS'13 and have obtained promising
results compared to several baseline methods. Moreover, on UCF Sports, we
demonstrate that action classifiers trained on these automatically obtained
spatio-temporal annotations have comparable performance to the classifiers
trained on ground truth annotation.
</p>
<h2><a href=http://arxiv.org/pdf/1605.05197v1.pdf>Towards Weakly-Supervised Action Localization</a></h2>
<p>  This paper presents a novel approach for weakly-supervised action
localization, i.e., that does not require per-frame spatial annotations for
training. We first introduce an effective method for extracting human tubes by
combining a state-of-the-art human detector with a tracking-by-detection
approach. Our tube extraction leverages the large amount of annotated humans
available today and outperforms the state of the art by an order of magnitude:
with less than 5 tubes per video, we obtain a recall of 95% on the UCF-Sports
and J-HMDB datasets. Given these human tubes, we perform weakly-supervised
selection based on multi-fold Multiple Instance Learning (MIL) with improved
dense trajectories and achieve excellent results. We obtain a mAP of 84% on
UCF-Sports, 54% on J-HMDB and 45% on UCF-101, which outperforms the state of
the art for weakly-supervised action localization and is close to the
performance of the best fully-supervised approaches.
  The second contribution of this paper is a new realistic dataset for action
localization, named DALY (Daily Action Localization in YouTube). It contains
high quality temporal and spatial annotations for 10 actions in 31 hours of
videos (3.3M frames), which is an order of magnitude larger than standard
action localization datasets. On the DALY dataset, our tubes have a spatial
recall of 82%, but the detection task is extremely challenging, we obtain 10.8%
mAP.
</p>
<h2><a href=http://arxiv.org/pdf/1511.07607v1.pdf>Fine-Grain Annotation of Cricket Videos</a></h2>
<p>  The recognition of human activities is one of the key problems in video
understanding. Action recognition is challenging even for specific categories
of videos, such as sports, that contain only a small set of actions.
Interestingly, sports videos are accompanied by detailed commentaries available
online, which could be used to perform action annotation in a weakly-supervised
setting. For the specific case of Cricket videos, we address the challenge of
temporal segmentation and annotation of ctions with semantic descriptions. Our
solution consists of two stages. In the first stage, the video is segmented
into "scenes", by utilizing the scene category information extracted from
text-commentary. The second stage consists of classifying video-shots as well
as the phrases in the textual description into various categories. The relevant
phrases are then suitably mapped to the video-shots. The novel aspect of this
work is the fine temporal scale at which semantic information is assigned to
the video. As a result of our approach, we enable retrieval of specific actions
that last only a few seconds, from several hours of video. This solution yields
a large number of labeled exemplars, with no manual effort, that could be used
by machine learning algorithms to learn complex actions.
</p>
<h2><a href=http://arxiv.org/pdf/1504.00983v2.pdf>Temporal Localization of Fine-Grained Actions in Videos by Domain
  Transfer from Web Images</a></h2>
<p>  We address the problem of fine-grained action localization from temporally
untrimmed web videos. We assume that only weak video-level annotations are
available for training. The goal is to use these weak labels to identify
temporal segments corresponding to the actions, and learn models that
generalize to unconstrained web videos. We find that web images queried by
action names serve as well-localized highlights for many actions, but are
noisily labeled. To solve this problem, we propose a simple yet effective
method that takes weak video labels and noisy image labels as input, and
generates localized action frames as output. This is achieved by cross-domain
transfer between video frames and web images, using pre-trained deep
convolutional neural networks. We then use the localized action frames to train
action recognition models with long short-term memory networks. We collect a
fine-grained sports action data set FGA-240 of more than 130,000 YouTube
videos. It has 240 fine-grained actions under 85 sports activities. Convincing
results are shown on the FGA-240 data set, as well as the THUMOS 2014
localization data set with untrimmed training videos.
</p>
<h2><a href=http://arxiv.org/pdf/1610.07031v2.pdf>Exercise Motion Classification from Large-Scale Wearable Sensor Data
  Using Convolutional Neural Networks</a></h2>
<p>  The ability to accurately observe human motion and identify human activities
is essential for developing automatic rehabilitation and sports training
systems. In this paper, large-scale exercise motion data obtained from a
forearm-worn wearable sensor are classified with a convolutional neural network
(CNN). Time series data consisting of accelerometer and orientation
measurements are formatted as "images", allowing the CNN to automatically
extract discriminative features. The resulting CNN classifies 50 gym exercises
with 92.14% accuracy. A comparative study on the effects of image formatting
and different CNN architectures is also presented.
</p>
<h2><a href=http://arxiv.org/pdf/1511.08177v1.pdf>Exploring Person Context and Local Scene Context for Object Detection</a></h2>
<p>  In this paper we explore two ways of using context for object detection. The
first model focusses on people and the objects they commonly interact with,
such as fashion and sports accessories. The second model considers more general
object detection and uses the spatial relationships between objects and between
objects and scenes. Our models are able to capture precise spatial
relationships between the context and the object of interest, and make
effective use of the appearance of the contextual region. On the newly released
COCO dataset, our models provide relative improvements of up to 5% over
CNN-based state-of-the-art detectors, with the gains concentrated on hard cases
such as small objects (10% relative improvement).
</p>
<h2><a href=http://arxiv.org/pdf/1112.2040v1.pdf>Recent Trends and Research Issues in Video Association Mining</a></h2>
<p>  With the ever-growing digital libraries and video databases, it is
increasingly important to understand and mine the knowledge from video database
automatically. Discovering association rules between items in a large video
database plays a considerable role in the video data mining research areas.
Based on the research and development in the past years, application of
association rule mining is growing in different domains such as surveillance,
meetings, broadcast news, sports, archives, movies, medical data, as well as
personal and online media collections. The purpose of this paper is to provide
general framework of mining the association rules from video database. This
article is also represents the research issues in video association mining
followed by the recent trends.
</p>
<h2><a href=http://arxiv.org/pdf/1112.2031v1.pdf>Learning Context for Text Categorization</a></h2>
<p>  This paper describes our work which is based on discovering context for text
document categorization. The document categorization approach is derived from a
combination of a learning paradigm known as relation extraction and an
technique known as context discovery. We demonstrate the effectiveness of our
categorization approach using reuters 21578 dataset and synthetic real world
data from sports domain. Our experimental results indicate that the learned
context greatly improves the categorization performance as compared to
traditional categorization approaches.
</p>
<h2><a href=http://arxiv.org/pdf/1411.1171v1.pdf>Multilinear Principal Component Analysis Network for Tensor Object
  Classification</a></h2>
<p>  The recently proposed principal component analysis network (PCANet) has been
proved high performance for visual content classification. In this letter, we
develop a tensorial extension of PCANet, namely, multilinear principal analysis
component network (MPCANet), for tensor object classification. Compared to
PCANet, the proposed MPCANet uses the spatial structure and the relationship
between each dimension of tensor objects much more efficiently. Experiments
were conducted on different visual content datasets including UCF sports action
video sequences database and UCF11 database. The experimental results have
revealed that the proposed MPCANet achieves higher classification accuracy than
PCANet for tensor object classification.
</p>
<h2><a href=http://arxiv.org/pdf/1612.06454v1.pdf>Exploring Structure for Long-Term Tracking of Multiple Objects in Sports
  Videos</a></h2>
<p>  In this paper, we propose a novel approach for exploiting structural
relations to track multiple objects that may undergo long-term occlusion and
abrupt motion. We use a model-free approach that relies only on annotations
given in the first frame of the video to track all the objects online, i.e.
without knowledge from future frames. We initialize a probabilistic Attributed
Relational Graph (ARG) from the first frame, which is incrementally updated
along the video. Instead of using the structural information only to evaluate
the scene, the proposed approach considers it to generate new tracking
hypotheses. In this way, our method is capable of generating relevant object
candidates that are used to improve or recover the track of lost objects. The
proposed method is evaluated on several videos of table tennis, volleyball, and
on the ACASVA dataset. The results show that our approach is very robust,
flexible and able to outperform other state-of-the-art methods in sports videos
that present structural patterns.
</p>
<h2><a href=http://arxiv.org/pdf/1611.09158v2.pdf>Spatio-Temporal Movements in Team Sports: A Visualization approach using
  Motion Charts</a></h2>
<p>  To analyze the movements and to study the trajectories of players is a
crucial need for a team when it looks to improve its chances of winning a match
or to understand its performances. State of the art tracking systems now
produce spatio-temporal traces of player trajectories with high definition and
frequency that has facilitated a variety of research efforts to extract insight
from the trajectories. Despite many methods borrowed from different disciplines
(machine learning, network and complex systems, GIS, computer vision,
statistics) has been proposed to answer to the needs of teams, a friendly and
easy-to-use approach to visualize spatio-temporal movements is still missing.
This paper suggests the use of gvisMotionChart function in GoogleVis R package.
I present and discuss results of a basketball case study. Data refers to a
match played by an italian team militant in "C-gold" league on March 22nd,
2016. With this case study I show that such a visualization approach could be
useful in supporting researcher on preliminar stages of their analysis on
sports' movements, and to facilitate the interpretation of their results.
</p>
<h2><a href=http://arxiv.org/pdf/1609.07495v1.pdf>A Rotation Invariant Latent Factor Model for Moveme Discovery from
  Static Poses</a></h2>
<p>  We tackle the problem of learning a rotation invariant latent factor model
when the training data is comprised of lower-dimensional projections of the
original feature space. The main goal is the discovery of a set of 3-D bases
poses that can characterize the manifold of primitive human motions, or
movemes, from a training set of 2-D projected poses obtained from still images
taken at various camera angles. The proposed technique for basis discovery is
data-driven rather than hand-designed. The learned representation is rotation
invariant, and can reconstruct any training instance from multiple viewing
angles. We apply our method to modeling human poses in sports (via the Leeds
Sports Dataset), and demonstrate the effectiveness of the learned bases in a
range of applications such as activity classification, inference of dynamics
from a single frame, and synthetic representation of movements.
</p>
<h2><a href=http://arxiv.org/pdf/1608.03793v2.pdf>Applying Deep Learning to Basketball Trajectories</a></h2>
<p>  One of the emerging trends for sports analytics is the growing use of player
and ball tracking data. A parallel development is deep learning predictive
approaches that use vast quantities of data with less reliance on feature
engineering. This paper applies recurrent neural networks in the form of
sequence modeling to predict whether a three-point shot is successful. The
models are capable of learning the trajectory of a basketball without any
knowledge of physics. For comparison, a baseline static machine learning model
with a full set of features, such as angle and velocity, in addition to the
positional data is also tested. Using a dataset of over 20,000 three pointers
from NBA SportVu data, the models based simply on sequential positional data
outperform a static feature rich machine learning model in predicting whether a
three-point shot is successful. This suggests deep learning models may offer an
improvement to traditional feature based machine learning methods for tracking
data.
</p>
<h2><a href=http://arxiv.org/pdf/1606.08955v1.pdf>Leveraging Contextual Cues for Generating Basketball Highlights</a></h2>
<p>  The massive growth of sports videos has resulted in a need for automatic
generation of sports highlights that are comparable in quality to the
hand-edited highlights produced by broadcasters such as ESPN. Unlike previous
works that mostly use audio-visual cues derived from the video, we propose an
approach that additionally leverages contextual cues derived from the
environment that the game is being played in. The contextual cues provide
information about the excitement levels in the game, which can be ranked and
selected to automatically produce high-quality basketball highlights. We
introduce a new dataset of 25 NCAA games along with their play-by-play stats
and the ground-truth excitement data for each basket. We explore the
informativeness of five different cues derived from the video and from the
environment through user studies. Our experiments show that for our study
participants, the highlights produced by our system are comparable to the ones
produced by ESPN for the same games.
</p>
<h2><a href=http://arxiv.org/pdf/1402.2363v1.pdf>Animation of 3D Human Model Using Markerless Motion Capture Applied To
  Sports</a></h2>
<p>  Markerless motion capture is an active research in 3D virtualization. In
proposed work we presented a system for markerless motion capture for 3D human
character animation, paper presents a survey on motion and skeleton tracking
techniques which are developed or are under development. The paper proposed a
method to transform the motion of a performer to a 3D human character (model),
the 3D human character performs similar movements as that of a performer in
real time. In the proposed work, human model data will be captured by Kinect
camera, processed data will be applied on 3D human model for animation. 3D
human model is created using open source software (MakeHuman). Anticipated
dataset for sport activity is considered as input which can be applied to any
HCI application.
</p>
<h2><a href=http://arxiv.org/pdf/1602.06994v1.pdf>Spatio-Temporal Analysis of Team Sports -- A Survey</a></h2>
<p>  Team-based invasion sports such as football, basketball and hockey are
similar in the sense that the players are able to move freely around the
playing area; and that player and team performance cannot be fully analysed
without considering the movements and interactions of all players as a group.
State of the art object tracking systems now produce spatio-temporal traces of
player trajectories with high definition and high frequency, and this, in turn,
has facilitated a variety of research efforts, across many disciplines, to
extract insight from the trajectories. We survey recent research efforts that
use spatio-temporal data from team sports as input, and involve non-trivial
computation. This article categorises the research efforts in a coherent
framework and identifies a number of open research questions.
</p>
<h2><a href=http://arxiv.org/pdf/1512.07502v1.pdf>Convolutional Architecture Exploration for Action Recognition and Image
  Classification</a></h2>
<p>  Convolutional Architecture for Fast Feature Encoding (CAFFE) [11] is a
software package for the training, classifying, and feature extraction of
images. The UCF Sports Action dataset is a widely used machine learning dataset
that has 200 videos taken in 720x480 resolution of 9 different sporting
activities: diving, golf, swinging, kicking, lifting, horseback riding,
running, skateboarding, swinging (various gymnastics), and walking. In this
report we report on a caffe feature extraction pipeline of images taken from
the videos of the UCF Sports Action dataset. A similar test was performed on
overfeat, and results were inferior to caffe. This study is intended to explore
the architecture and hyper parameters needed for effective static analysis of
action in videos and classification over a variety of image datasets.
</p>
<h2><a href=http://arxiv.org/pdf/astro-ph/0201205v1.pdf>SDAMS: SPOrt Data Archiving and Management System</a></h2>
<p>  SDAMS is the ensemble of database + software packages aimed to the archiving,
quick-look analysis, off-line analysis, network accessibility and plotting of
the SPOrt produced data. Many of the aspects related to data archiving,
analysis and distribution are common to almost all the astronomical
experiments. SDAMS ambition is to face and solve problems like accessibility
and portability of the data on any hardware/software platform in a way as
simpler as possible, though effective. The system is conceived in a way to be
used either by the scientific community interested in background radiation
studies or by a wider public with low or null knowledge of the subject. The
user authentication system allows us to apply different levels of access,
analysis and data retrieving. SDAMS will be accessible through any Web browser
though the most efficient way to use it is by writing simple programs. Graphics
and images useful for outreach purposes will be produced and put on the Web on
a regular basis.
</p>
<h2><a href=http://arxiv.org/pdf/1509.06279v1.pdf>Sports highlights generation based on acoustic events detection: A rugby
  case study</a></h2>
<p>  We approach the challenging problem of generating highlights from sports
broadcasts utilizing audio information only. A language-independent,
multi-stage classification approach is employed for detection of key acoustic
events which then act as a platform for summarization of highlight scenes.
Objective results and human experience indicate that our system is highly
efficient.
</p>
<h2><a href=http://arxiv.org/pdf/1404.6413v1.pdf>Indoor Activity Detection and Recognition for Sport Games Analysis</a></h2>
<p>  Activity recognition in sport is an attractive field for computer vision
research. Game, player and team analysis are of great interest and research
topics within this field emerge with the goal of automated analysis. The very
specific underlying rules of sports can be used as prior knowledge for the
recognition task and present a constrained environment for evaluation. This
paper describes recognition of single player activities in sport with special
emphasis on volleyball. Starting from a per-frame player-centered activity
recognition, we incorporate geometry and contextual information via an activity
context descriptor that collects information about all player's activities over
a certain timespan relative to the investigated player. The benefit of this
context information on single player activity recognition is evaluated on our
new real-life dataset presenting a total amount of almost 36k annotated frames
containing 7 activity classes within 6 videos of professional volleyball games.
Our incorporation of the contextual information improves the average
player-centered classification performance of 77.56% by up to 18.35% on
specific classes, proving that spatio-temporal context is an important clue for
activity recognition.
</p>
<h2><a href=http://arxiv.org/pdf/1307.7198v1.pdf>Self-Learning for Player Localization in Sports Video</a></h2>
<p>  This paper introduces a novel self-learning framework that automates the
label acquisition process for improving models for detecting players in
broadcast footage of sports games. Unlike most previous self-learning
approaches for improving appearance-based object detectors from videos, we
allow an unknown, unconstrained number of target objects in a more generalized
video sequence with non-static camera views. Our self-learning approach uses a
latent SVM learning algorithm and deformable part models to represent the shape
and colour information of players, constraining their motions, and learns the
colour of the playing field by a gentle Adaboost algorithm. We combine those
image cues and discover additional labels automatically from unlabelled data.
In our experiments, our approach exploits both labelled and unlabelled data in
sparsely labelled videos of sports games, providing a mean performance
improvement of over 20% in the average precision for detecting sports players
and improved tracking, when videos contain very few labelled images.
</p>
<h2><a href=http://arxiv.org/pdf/1703.01170v1.pdf>A Survey on Content-Aware Video Analysis for Sports</a></h2>
<p>  Sports data analysis is becoming increasingly large-scale, diversified, and
shared, but difficulty persists in rapidly accessing the most crucial
information. Previous surveys have focused on the methodologies of sports video
analysis from the spatiotemporal viewpoint instead of a content-based
viewpoint, and few of these studies have considered semantics. This study
develops a deeper interpretation of content-aware sports video analysis by
examining the insight offered by research into the structure of content under
different scenarios. On the basis of this insight, we provide an overview of
the themes particularly relevant to the research on content-aware systems for
broadcast sports. Specifically, we focus on the video content analysis
techniques applied in sportscasts over the past decade from the perspectives of
fundamentals and general review, a content hierarchical model, and trends and
challenges. Content-aware analysis methods are discussed with respect to
object-, event-, and context-oriented groups. In each group, the gap between
sensation and content excitement must be bridged using proper strategies. In
this regard, a content-aware approach is required to determine user demands.
Finally, the paper summarizes the future trends and challenges for sports video
analysis. We believe that our findings can advance the field of research on
content-aware video analysis for broadcast sports.
</p>
